## 활성화 함수

입력 신호 -> 출력 신호로 변환하는데 이 때 활성화 여부를 결정하기 때문에 활성화 함수라고 부른다고 함.

```
y = h(b + wx + wy)
```

### 활성화 함수는 비선형함수여야 한다

선형함수를 생각해보면 중첩을 계속 해도 그대로 수식으로 표현이 가능함.
즉, 가중치 조절만 잘하면 하나의 수식으로 끝나고 은닉층을 중첩하는 의미가 없음.

```
y = cx
```

데이터를 선형적으로 보면서 데이터의 형태나 성격에 따라 한계가 있다고 느끼면서 비선형적으로 분석해보고 싶다고 생각했는데  
그런 좋은 대안이 될까 싶음. 일단 더 공부해보고 해봐야 감이 잡힐듯.

### 계단 함수

기중치(w)와 편향(b)를 통해 출력값이 1 또는 0으로 정해질때 아래와 같은 방식은 0을 임계점으로 출력값이 정해지기 계단형으로 정해지기 때문에 계단함수라고 부른다고 함.

```
h(x) = x <= 0 ? 0 : 1
```

### 시그모이드 함수

함수를 시각화해서 보면 알 수 있지만 활성화 단계에 연속적으로 값을 가짐.  
이 출력의 연속성이 신경망 학습에 중요한 역할을 한다고 하는데 아직 잘 모르겠음. (나중에 알겠지)

```
h(x) = 1 / (1 + exp(-x))

```

### ReLU 함수

시그모이드 함수가 오랬동안 이용되어왔지만 지금은 ReLU가 쓰인다고 함
(이 책이 쓰여진 시점이 2년전이라 지금은 또 어떨지 몰라서 나중에 찾아봐야할듯. 기초만 익히자)

```
h(x) = x > 0 ? x : 0
```
