## 1. 활성화 함수

입력 신호 -> 출력 신호로 변환하는데 이 때 활성화 여부를 결정하기 때문에 활성화 함수라고 부른다고 함.

```
y = h(b + wx + wy)
```

### 활성화 함수는 비선형함수여야 한다

선형함수를 생각해보면 중첩을 계속 해도 그대로 수식으로 표현이 가능함.
즉, 가중치 조절만 잘하면 하나의 수식으로 끝나고 은닉층을 중첩하는 의미가 없음.

```
y = cx
```

데이터를 선형적으로 보면서 데이터의 형태나 성격에 따라 한계가 있다고 느끼면서 비선형적으로 분석해보고 싶다고 생각했는데  
그런 좋은 대안이 될까 싶음. 일단 더 공부해보고 해봐야 감이 잡힐듯.

### 계단 함수

기중치(w)와 편향(b)를 통해 출력값이 1 또는 0으로 정해질때 아래와 같은 방식은 0을 임계점으로 출력값이 정해지기 계단형으로 정해지기 때문에 계단함수라고 부른다고 함.

```
h(x) = x <= 0 ? 0 : 1
```

### 시그모이드 함수

함수를 시각화해서 보면 알 수 있지만 활성화 단계에 연속적으로 값을 가짐.  
이 출력의 연속성이 신경망 학습에 중요한 역할을 한다고 하는데 아직 잘 모르겠음. (나중에 알겠지)

```
h(x) = 1 / (1 + exp(-x))

```

### ReLU 함수

시그모이드 함수가 오랬동안 이용되어왔지만 지금은 ReLU가 쓰인다고 함.
(이 책이 쓰여진 시점이 2년전이라 지금은 또 어떨지 몰라서 나중에 찾아봐야할듯. 기초만 익히자)

```
h(x) = x > 0 ? x : 0
```

## 2. 행렬 곱으로 신경망 계산하기

행렬곱 방식이랑 신경망에서 입출력 얻는 방식을 잘 생각해보면 결국 행렬곱임.
x1, x2 -> y1, y2, y3 로 간다고 생각해보자.

```python
x = np.array([[1, 2]])
w = np.array([[1, 3, 5], [2, 4, 6]])
y = np.dot(x, w) # [[5, 11, 17]]
```

## 3. 다층 신경망 구현하기

x1,x2 -> 3개 노드 -> 2개 노드 -> y1,y2  
신경망이 있다고 생각해보자.  
오 이거 아이디어 좋다.. 현재 반려동물의 여러가지 배변패턴으로 개체를 구분하려 하는데 써먹을 수 있을 것 같다.  
결국은 가중치랑 편향을 잘 정해야할 것 같고, 이 값을 이리저리 바꿔가면서 테스트 해볼 수 있는 환경이 중요할 것 같다.  
또 더 나은 가중치를 더 빠르게 얻을 수 있는 알고리즘 같은 것들이 고안되었을 것 같다. 찾아보자.  

ex. 개체마다 기존 배변량, 배변시간, 시간대, ... 등의 평균이나 편차같은 어떤 값을 가중치로 두고 y 값을 얻은 뒤 y 값이 가장 높은 개체가 가장 가까운 개체로 판단하는 방식(?) 가능하지 않을까  

```python
# 입력층
x = np.array([[1.0, 0.5]])

# 은닉 1층
w1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
b1 = np.array([[0.1, 0.2, 0.3]])
a1 = np.dot(x, w1) + b1
z1 = sigmoid(a1) # 1x3 => 새로운 입력층 (3개 노드)

# 은닉 2층
w2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
b2 = np.array([[0.1, 0.2]])
a2 = np.dot(z1, w2) + b2
z2 = sigmoid(a2) # 1x2 => 새로운 입력층 (2개 노드)

# 은닉 3층
w3 = np.array([[0.1, 0.3], [0.2, 0.4]])
b3 = np.array([[0.1, 0.2]])
a3 = np.dot(z2, w3) + b3
y = identity_function(a3) # 출력값을 식별함수를 이용해서 결과를 결정한다.
```

# 4. 출력층 설계하기

일반적으로 회귀 -> 항등함수, 분류 -> 소프트맥스 함수를 사용함.

## 항등함수

입력을 그대로 출력하는 함수  
따라서 연속성 있는 y를 얻어야할 때 좋을듯 (회귀분석)

## 소프트맥스함수

가장 중요한 특징

1. 0과 1 사이의 실수
2. 출력의 총합이 1이됨

확률로 해석이 가능함 -> 분류 알고리즘에 유용함
'얘일 확률이 제일 높다'

그리고 주의해야할 점은 지수함수를 이용해서 값이 너무커질경우 컴퓨터는 부정확해지기 때문에 (최대 표현수의 한계) C값을 잘 정해야함.

원래 소프트맥스함수는

`y = exp(a) / sum(exp(a))` 인데.. exp(a)가 너무 커질 수 있으니 식을 좀 수정하면 이런식으로 C를 정할 수 있겠지  
`y = (C * exp(a)) / (C * sum(exp(a)))`  
`y = exp(a + logC) / sum(exp(a + logC))`  
`y = exp(a + C) / sum(exp(a + C))`

- **C는 입력신호 중 최대값을 사용하는게 일반적**
- **소프트맥스 함수는 확률로 표현을 해주는거지 없어도 분류 score 매기는데는 문제가 없어서 생략하기도 함**

```python
# 소프트맥스 함수 구현은 main.py에 해둠
```
